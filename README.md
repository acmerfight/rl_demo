# RL-Demo 强化学习演示

一个简洁明了的强化学习算法演示项目，用于展示和解释几种经典的强化学习算法原理及其Python实现。

## 项目概述

本项目旨在通过简单直观的环境和详细的代码注释，帮助初学者和爱好者理解强化学习的核心概念和算法实现。项目实现了三种经典的强化学习算法：

1. **Q-learning**：基于表格的值迭代方法，适用于离散状态和动作空间
2. **策略梯度（Policy Gradient）**：直接优化策略的方法，适用于离散动作空间
3. **连续策略梯度（Continuous Policy Gradient）**：适用于连续动作空间的策略梯度变体

每个算法都配有详细的中文注释和解释，帮助用户理解算法的工作原理和实现细节。

## 算法原理简述

### Q-learning
Q-learning是一种基于值函数的强化学习算法，通过构建和优化Q表（状态-动作值函数）来学习最优策略。核心更新公式：

```
Q(s,a) ← Q(s,a) + α[r + γ·max Q(s',a') - Q(s,a)]
```

其中：
- α：学习率，控制新信息的接受程度
- γ：折扣因子，控制对未来奖励的重视程度
- r：即时奖励
- s和a：当前状态和动作
- s'：下一状态

### 策略梯度（Policy Gradient）
策略梯度算法直接对策略函数进行参数化和优化，使用梯度上升方法最大化期望回报。本项目实现了REINFORCE算法，核心思想是：策略梯度正比于动作概率的对数与回报的乘积。

### 连续策略梯度（Continuous Policy Gradient）
连续策略梯度算法扩展了策略梯度方法，用于处理连续动作空间的问题。本项目通过参数化高斯分布（均值和方差）来表示连续动作策略，使智能体能在二维空间中寻找目标。

## 功能特点

- **直观的环境**：
  - 简单迷宫环境（离散状态和动作空间）
  - 二维目标寻找环境（连续状态和动作空间）

- **算法实现**：
  - 表格型 Q-learning 算法
  - REINFORCE 策略梯度算法
  - 连续动作空间的策略梯度算法

- **可视化功能**：
  - 环境状态的实时可视化
  - 智能体行为的直观展示
  - 训练过程的进度和性能统计

- **教学功能**：
  - 详细的算法解释函数
  - 算法更新步骤的分步解释
  - 丰富的中文注释和文档

## 演示效果

### Q-learning 迷宫求解
Q-learning算法在迷宫环境中的效果：智能体初始位于位置A，通过探索环境学习到避开陷阱C并到达目标D的最优路径。训练过程中，Q表逐渐收敛，智能体从随机探索转向利用已知信息。

### 策略梯度迷宫求解
策略梯度算法通过直接优化策略参数，使智能体学习到从起点到终点的概率分布。随着训练进行，到达目标状态的概率不断提高。

### 连续动作空间目标寻找
在二维环境中，智能体学习通过连续的动作（方向和力度）从随机起点移动到固定目标位置（8,8）。训练后，智能体能够沿近似直线路径高效到达目标。

## 安装指南

### 环境要求

- Python >= 3.13

### 安装步骤

1. 克隆仓库

```bash
git clone https://github.com/用户名/rl-demo.git
cd rl-demo
```

2. 创建虚拟环境并安装依赖

```bash
# 使用 venv
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# 或 .venv\Scripts\activate  # Windows

# 使用 uv 安装依赖
pip install uv
uv pip install .
```

## 使用方法

### Q-learning 算法演示

```bash
python q_learning.py
```

### 策略梯度算法演示（离散动作空间）

```bash
python policy_gradient.py
```

### 连续策略梯度算法演示

```bash
python continuous_policy_gradient.py
```

## 参数调整建议

### Q-learning
- **学习率(α)**：通常设置在0.1-0.5之间。值越大，学习越快但可能不稳定
- **折扣因子(γ)**：通常设置在0.9-0.99之间。值越大，越重视长期回报
- **探索率(ε)**：建议从0.1-0.3开始，可随训练进度逐渐降低

### 策略梯度
- **学习率**：建议从0.01-0.1开始，视收敛情况调整
- **折扣因子**：0.9-0.999之间，连续环境通常需要更高的值

### 连续策略梯度
- **学习率**：连续动作空间通常需要较小的学习率，0.001-0.01
- **方差初始值**：控制探索程度，建议从1.0开始，随训练进度可逐渐减小

## 项目文件结构

- `q_learning.py` - Q-learning 算法实现与演示
- `policy_gradient.py` - 策略梯度算法实现与演示
- `continuous_policy_gradient.py` - 连续动作空间的策略梯度算法实现与演示
- `main.py` - 项目主入口文件

## 学习资源

通过本项目，您可以学习：

1. 强化学习的基本概念：状态、动作、奖励、策略、值函数等
2. Q-learning 算法的数学原理和实现方法
3. 策略梯度算法的理论基础和实现技巧
4. 连续动作空间问题的处理方法
5. 强化学习算法的调试和可视化技术

## 常见问题 (FAQ)

1. **Q: 为什么训练初期智能体表现很差？**  
   A: 这是正常现象。强化学习需要探索环境来获取经验，初期以随机行为为主。

2. **Q: 如何加快训练速度？**  
   A: 可以调整奖励函数使其更加稀疏，增加学习率，或减少环境复杂度。

3. **Q: 训练不稳定或无法收敛怎么办？**  
   A: 可以尝试降低学习率，增加训练轮数，或检查奖励设计是否合理。

## 贡献指南

欢迎对本项目进行贡献！您可以通过以下方式参与：

- 提交 Bug 报告或功能建议
- 改进代码或文档
- 添加新的算法实现或环境

## 许可证

本项目采用 MIT 许可证。详情请参阅 LICENSE 文件。
